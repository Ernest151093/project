{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "### [1. Подготовка](#section_id1)\n",
    "### [2. Обучение](#section_id2)\n",
    "### [3. Выводы](#section_id3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_id1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датафрейм состоит только из двух столбцов: 1-ый - тексты, 2-ой целевой признак. Перед тем как проводить обучение модели, необходимо провести лемматизацию, удалить ненужные слова (как правило это союзы предлоги и всякие частички), посчитаем вел-ну TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем столбец text в список текстов. Переведём тексты в стандартный для Python формат: кодировку Unicode (U)\n",
    "corpus = df['text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clear = []\n",
    "new_lemm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#проведем чистку \n",
    "def clear_text(text):\n",
    "    for i in range(len(text)):\n",
    "        clear_text = re.sub(r'[^a-zA-Z ]', ' ', text[i])\n",
    "        new_clear.append(\" \".join(clear_text.split()))\n",
    "        \n",
    "    return new_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#проведем лемматизацию слов\n",
    "def lemmatize(text):\n",
    "    #print(text)\n",
    "    for i in text:\n",
    "        word_list = nltk.word_tokenize(i)\n",
    "        lemmatized_output = \" \".join([lemmatizer.lemmatize(j) for j in word_list])\n",
    "        new_lemm.append(lemmatized_output)\n",
    "    \n",
    "    return new_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemm_text'] = clear_text(lemmatize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_id2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state = 12345, C=2.5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['lemm_text']\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.2)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, target_valid, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656,)\n",
      "(127656,)\n",
      "(15957,)\n",
      "(15957,)\n",
      "(15958,)\n",
      "(15958,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(target_valid.shape)\n",
    "print(features_test.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train = count_tf_idf.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.44 s, sys: 3.96 s, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=12345, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(tf_idf_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7581512002866356\n"
     ]
    }
   ],
   "source": [
    "tf_idf_valid = count_tf_idf.transform(features_valid)\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "print(f1_score(target_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7613793103448275\n"
     ]
    }
   ],
   "source": [
    "tf_idf_test = count_tf_idf.transform(features_test)\n",
    "predictions = model.predict(tf_idf_test)\n",
    "print(f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Валидационная выборка модели \"Логистическая регрессия\" показала f1-метрику = 0,756, а тестовая даже лучше 0,762. Данная метрика нас полностью удовлетворяет. Но ради интереса посмотрим, как поведет себя  метрика если мы исключим дисбаланс классов целевых признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#напишем функцию дисбаланса\n",
    "def class_balance (features_train, target_train):\n",
    "    features_zeros = features_train[target_train == 0]\n",
    "    features_ones = features_train[target_train == 1]\n",
    "    target_zeros = target_train[target_train == 0]\n",
    "    target_ones = target_train[target_train == 1]\n",
    "    return (features_zeros, features_ones, target_zeros, target_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114742,)\n",
      "(12914,)\n",
      "(114742,)\n",
      "(12914,)\n"
     ]
    }
   ],
   "source": [
    "features_zeros, features_ones, target_zeros, target_ones = class_balance(features_train, target_train)\n",
    "print(features_zeros.shape)\n",
    "print(features_ones.shape)\n",
    "print(target_zeros.shape)\n",
    "print(target_ones.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#напишем функцию, которая увеличивает положительную выборку (новые не создаются, а просто дублируются существующие\n",
    "#и перемешиваются)\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros, features_ones, target_zeros, target_ones = class_balance (features, target)\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    #print(target_upsampled.value_counts())\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=123456)\n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243882,)\n",
      "(243882,)\n"
     ]
    }
   ],
   "source": [
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 10)#так как отношение 1/10\n",
    "print(features_upsampled.shape)\n",
    "print(target_upsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf_sample = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train_sample = count_tf_idf_sample.fit_transform(features_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=12345, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_idf_train_sample, target_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7585235277542969\n"
     ]
    }
   ],
   "source": [
    "tf_idf_valid = count_tf_idf_sample.transform(features_valid)\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "print(f1_score(target_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7550585729499468\n"
     ]
    }
   ],
   "source": [
    "tf_idf_test = count_tf_idf_sample.transform(features_test)\n",
    "predictions = model.predict(tf_idf_test)\n",
    "print(f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При устранении дисбалансов класса путем увеличения положительных классов в 10 раз (мы просто дублировали, нового ничего не добавляли), валидационная метрика стала лучше 0,765, и на тестовой выборке качество 0,763."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим модель случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(random_state = 12345, n_estimators = 100, max_depth = 50)\n",
    "model.fit(tf_idf_train_sample, target_upsampled)\n",
    "predictions_valid = model.predict(tf_idf_valid)\n",
    "print(f'max_depth = 100:', end=' ')\n",
    "print(f'n_estimators = 100:', end=' ')\n",
    "print('f1 score =', f1_score(target_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, модель случайный лес очень долго обучается и f1-метрика не соответствует заданным требованиям. (Используя более высокие значения гиперпараметров, может и можно добиться нужной метрики но это займет очень большое кол-во времени) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_id3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нам был предоставлен датафрейм, состоящий из двух признаков: текст и признак определяющий токсичные комментарии. Перед началом обучения, была проведена очистка текстов от цифр, знаков препинания, подчеркиваний и т.д. После чего мы провели лемматизацию текста (привели в исходное состояние текста). Далее мы еще избавились от левых слов (типа союза предлога и т.д.). Использовалась модель логистическая регрессия, случайный лес и я честно пытался попробовать модель градиентного бустинга и BERT (но кернел все время умирал, что удаленно, что и локально в юпитере). Перед началом обучения мы перевели текст в векторное представление путев TF-IDF. Также наша выборка была разбита на 3 датасета (трейн, валид и тест). В итоге логистическая регрессия на валидационной и тестовой выборках показали f1-метрику выше 0,75, что соответствует заданию. Также я решил попробовать увеличить метрику путем исключения дисбаланса классов путем увеличения положительных значений (1 - положительная, 0 - отрицательная). В итоге качество улучшилось, но не намного, что не в прочем не имеет смысла усложнять работы и можно обойтись без операций дисбаланса."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
